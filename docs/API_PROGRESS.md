# Analysis Agent Progress Tracking

## How to Check Analysis Progress

### 1. Get Detailed Analysis Agent Progress
**Endpoint:** `GET /api/agents/analysis/progress`

**Response:**
```json
{
  "agent_name": "Analysis Agent",
  "status": "busy",
  "progress": 45,
  "current_task": "task_1234567890_0",
  "task_details": {
    "task_id": "task_1234567890_0",
    "task_type": "statistical_analysis",
    "dataset_type": "thesis",
    "cleaned_file": "data/processed/thesis_20251108_000646.csv",
    "created_at": "2025-11-08T00:06:46",
    "started_at": "2025-11-08T00:06:47"
  },
  "recent_logs": [
    {
      "timestamp": "2025-11-08T00:06:47",
      "level": "info",
      "message": "Starting analysis task: statistical_analysis"
    },
    {
      "timestamp": "2025-11-08T00:06:48",
      "level": "info",
      "message": "Loading cleaned data from: data/processed/thesis_20251108_000646.csv"
    }
  ],
  "has_results": false
}
```

### 2. Get All Agents Status (including progress)
**Endpoint:** `GET /api/agents`

**Response:**
```json
{
  "analysis_agent": {
    "name": "Analysis Agent",
    "status": "busy",
    "progress": 45,
    "current_task": "task_1234567890_0",
    "capabilities": ["statistical_analysis", "trend_analysis", "correlation_analysis"]
  },
  ...
}
```

### 3. Get Specific Task Status
**Endpoint:** `GET /api/tasks/<task_id>/status`

**Response:**
```json
{
  "task_id": "task_1234567890_0",
  "status": "in_progress",
  "progress": 45,
  "assigned_agent": "analysis_agent",
  "agent_status": "busy",
  "created_at": "2025-11-08T00:06:46",
  "started_at": "2025-11-08T00:06:47"
}
```

### 4. Get All Tasks
**Endpoint:** `GET /api/tasks`

**Response:**
```json
{
  "queued": [],
  "active": [
    {
      "id": "task_1234567890_0",
      "type": "statistical_analysis",
      "status": "in_progress",
      "assigned_agent": "analysis_agent",
      ...
    }
  ],
  "completed": [...]
}
```

## Progress States

- **0%**: Task just started
- **10%**: Task initialization
- **20%**: Loading cleaned data
- **30%**: Starting analysis
- **80%**: Analysis completed, storing results
- **100%**: Task completed

## Example: Checking Progress in Browser

1. Open browser console or use curl:
```bash
# Check analysis agent progress
curl http://localhost:5000/api/agents/analysis/progress

# Check all agents
curl http://localhost:5000/api/agents

# Check specific task (replace with your task_id)
curl http://localhost:5000/api/tasks/task_1234567890_0/status
```

2. Or visit in browser:
- http://localhost:5000/api/agents/analysis/progress
- http://localhost:5000/api/agents
- http://localhost:5000/api/tasks

## Real-time Progress Updates

The progress is updated in real-time as the analysis progresses. You can poll these endpoints to get live updates.

## Orchestration Graph Inspection

- **Endpoint:** `GET /api/graph/structure`
- **Description:** Returns metadata describing the LangGraph orchestration pipeline. Includes node/edge definitions and a Mermaid diagram string for visualization.
- **Response:**
  ```json
  {
    "available": true,
    "nodes": [
      {"key": "data_agent", "label": "Data Agent", "description": "..."}
    ],
    "edges": [
      {"source": "data_agent", "target": "analysis_agent"}
    ],
    "mermaid": "stateDiagram-v2 ...",
    "graph_summary": {"total_nodes": 4, "total_edges": 4}
  }
  ```
- **Notes:** When LangGraph is not installed on the backend, `available` is `false` and the payload includes a message explaining how to enable the feature. The frontend analysis dashboard uses the Mermaid payload to render a live diagram of the agent pipeline.

## Orchestration Graph Execution

- **Endpoint:** `POST /api/graph/run`
- **Description:** Executes the LangGraph pipeline (Data → Analysis → Visualization → Report) for a supplied dataset. Accepts either a raw `input_file` (path to uploaded CSV) or a `cleaned_file` generated by the data agent. When LangGraph is not installed, the endpoint returns HTTP 501.
- **Request Body:**
  ```json
  {
    "input_file": "../data/uploads/20251108_140830_seatac.csv",
    "dataset_type": "thesis"
  }
  ```
  or
  ```json
  {
    "cleaned_file": "../data/processed/thesis_20251108_140830.csv",
    "dataset_type": "thesis",
    "cleaning_stats": {"rows_before": 500, "rows_after": 480}
  }
  ```
- **Response:**
  ```json
  {
    "dataset_type": "thesis",
    "cleaned_file": ".../data/processed/thesis_20251108_140830.csv",
    "analysis_result": {"dataset_overview": {...}},
    "visualization_summary": {"row_count": 480, "columns": [...]},
    "report_summary": {...},
    "status": "completed",
    "logs": [
      {"node": "data_agent", "message": "Starting data cleaning step"},
      {"node": "analysis_agent", "message": "Starting statistical analysis"}
    ]
  }
  ```
- **Notes:** The pipeline reuses the existing agent implementations. Each node runs in a background thread (`asyncio.to_thread`) so CPU-bound cleaning and analysis steps do not block the event loop. Coordinator results (`latest_cleaning`, `<dataset>_uploaded_analysis`, etc.) are updated as the graph advances, ensuring the UI stays in sync.



